// Teste b√°sico da API OpenAI
// Execute: node test-openai.js

import OpenAI from "openai";
import dotenv from "dotenv";

// Carregar vari√°veis de ambiente
dotenv.config();

// Configurar cliente OpenAI
// A API key j√° cont√©m informa√ß√µes de organiza√ß√£o e projeto
// N√£o precisamos especificar manualmente se estiver causando conflitos
const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY?.trim(), // Remove espa√ßos em branco
  // N√£o especificar organization/project aqui - deixar a API key gerenciar
});

async function listAvailableModels() {
  try {
    console.log("üìã Listando modelos dispon√≠veis...");
    const models = await client.models.list();
    const availableModels = models.data
      .filter(m => m.id.includes('gpt') || m.id.includes('o1'))
      .map(m => m.id)
      .sort();
    
    console.log("‚úÖ Modelos dispon√≠veis:");
    availableModels.forEach(model => console.log(`   - ${model}`));
    return availableModels;
  } catch (error) {
    console.log("‚ö†Ô∏è  N√£o foi poss√≠vel listar modelos:", error.message);
    return [];
  }
}

async function testOpenAI() {
  try {
    console.log("üîÑ Testando API OpenAI...");
    console.log("üîë API Key:", process.env.OPENAI_API_KEY?.substring(0, 20) + "...");
    console.log("");
    
    // Listar modelos dispon√≠veis primeiro
    const availableModels = await listAvailableModels();
    console.log("");
    
    // Tentar modelos em ordem de prefer√™ncia
    const modelsToTry = availableModels.length > 0 
      ? availableModels.slice(0, 3) // Usar os 3 primeiros dispon√≠veis
      : ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4", "o1-mini", "o1-preview"]; // Fallback
    
    let response = null;
    let modelUsed = null;
    let lastError = null;

    for (const model of modelsToTry) {
      try {
        console.log(`üîÑ Tentando modelo: ${model}...`);
        response = await client.chat.completions.create({
          model: model,
          messages: [
            {
              role: "user",
              content: "Write a one-sentence bedtime story about a unicorn."
            }
          ],
          temperature: 0.7,
          max_tokens: 100
        });
        modelUsed = model;
        break;
      } catch (error) {
        lastError = error;
        if (error.status === 403 && (error.code === 'model_not_found' || error.message?.includes('does not have access'))) {
          console.log(`   ‚ùå Modelo ${model} n√£o dispon√≠vel neste projeto`);
          continue;
        }
        throw error;
      }
    }

    if (!response) {
      throw lastError || new Error("Nenhum modelo dispon√≠vel");
    }

    console.log("\n‚úÖ Sucesso!");
    console.log("üì§ Resposta:", response.choices[0]?.message?.content);
    console.log("\nüìä Detalhes:");
    console.log("  - Modelo usado:", modelUsed || response.model);
    console.log("  - Tokens usados:", response.usage?.total_tokens);
    console.log("  - Prompt tokens:", response.usage?.prompt_tokens);
    console.log("  - Completion tokens:", response.usage?.completion_tokens);
    
  } catch (error) {
    console.error("\n‚ùå Erro:", error.message);
    
    if (error.status === 401) {
      console.error("‚ö†Ô∏è  API key inv√°lida ou expirada");
      console.error("   Verifique se OPENAI_API_KEY est√° correto no .env");
    } else if (error.status === 403) {
      console.error("‚ö†Ô∏è  Acesso negado ao modelo");
      console.error("   Poss√≠veis causas:");
      console.error("   1. O projeto n√£o tem acesso ao modelo solicitado");
      console.error("   2. A API key est√° associada a um projeto diferente");
      console.error("   3. Tente remover PROJECT_ID do .env ou usar outro projeto");
    } else if (error.status === 429) {
      console.error("‚ö†Ô∏è  Rate limit excedido");
    } else {
      console.error("   Detalhes:", error);
    }
    process.exit(1);
  }
}

testOpenAI();
